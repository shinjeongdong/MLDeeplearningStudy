{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "16-C9zT3X7VRswt9iCvva9COBWyXQEH24",
      "authorship_tag": "ABX9TyOraPmL+SVaQ+UGuUSOhPzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinjeongdong/MLDeeplearningStudy/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%94%A5%EB%9F%AC%EB%8B%9D_4%EC%9E%A5_%EC%8B%A0%EA%B2%BD%EB%A7%9D_%ED%95%99%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHPjjfG1pTI2"
      },
      "outputs": [],
      "source": [
        "#2025-05-16\n",
        "#여기서 학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획듯하는 것을 뜻함.\n",
        "#이번 장에서는 신경망이 학습할 수 있도록 해주는 지표인 손실 함수를 소개함.\n",
        "#이 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는게 이번 챕터의 목표\n",
        "import numpy as np\n",
        "import importlib\n",
        "importlib.invalidate_caches()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#손실함수\n",
        "#오차 제곱합 == E = 1/2 *np.sum((y - t)**2)"
      ],
      "metadata": {
        "id": "Rf4UAT7Ar4K9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#예시\n",
        "\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] #예측값\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] #정답값\n",
        "#이 배열의 원소는 첫 번째 인덱스부터 순서대로 숫자 0 1 2 일 때의 값\n",
        "#여기에서 ㅣㄴ경망의 출력 y는 softmax함수의 출력 고로 확률값\n",
        "#이 예시에서 이미지가 0일확률은 0.1 1일 확률은 0.05 2일 확률은 0.6\n",
        "#정답 레이블인 t는 정답을 가리키는 위치의 원소는 1로 그 외에는 0으로 ㅍ기함 여기에서는 숫자 2에 해당되는 원소의 값이\n",
        "#1이므로 정답이 2임을 알수있다./"
      ],
      "metadata": {
        "id": "pfrIC3VnsVGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#손실함수\n",
        "#오차제곱합\n",
        "def sum_squares_error(y,t):\n",
        "  return 0.5 * np.sum((y-t)**2)\n",
        "\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] #예측값\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] #정답값\n",
        "\n",
        "#예 : 2일 확률이 가장 높다고 추정함 (0.6)\n",
        "print(sum_squares_error(np.array(y),np.array(t))) # 0.0975 오차값이 낮게 나온 모습\n",
        "\n",
        "#예2 : 7일 확률이 가장 높다고 추정함 (0.6)\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "print(sum_squares_error(np.array(y),np.array(t))) # 0.5975 오차가 높게 나온 모습."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOeFJe55tJ1o",
        "outputId": "74ddb25c-6435-485d-c5ff-91ee0e82678e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09750000000000003\n",
            "0.5975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#교차 엔트로피 오차 cross entropy error CEE\n",
        "#여기서 log는 자연로그 y는 신경망의 출력, t는 정답 레이블\n",
        "#또 t는 정답에 해당하는 인덱스의 원소만 1이고 다른 건 0임. 그래서 실적으로 정답일 때 (t가 1일때의 y)의 자연로그를 계산함\n",
        "#만약 y랑 t둘다 1이면 0을 출력함 오차가 0이라는 뜻.\n",
        "#E = -sum.np(t * np.log(y))\n",
        "\n",
        "def cross_entropy_error(y,t):\n",
        "  y = np.array(y)\n",
        "  t = np.array(t)\n",
        "  delta = 1e-7\n",
        "  return -np.sum(t * np.log(y+delta))\n",
        "#아주 작은 값인 delta를 더한 이유는 np.log 함수에 0을 입력하면 -inf가 출력되어\n",
        "#이를 방지 하기 위해\n",
        "\n",
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] #예측값\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] #정답값\n",
        "\n",
        "print(cross_entropy_error(y,t)) # 0.51\n",
        "\n",
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "\n",
        "print(cross_entropy_error(y,t)) # 2.30\n",
        "#밑의 오차가 2.3으로 훨씬 크게 나옴 앞서 오차제곱합의 판단과 일치함."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glxftLpBuMPr",
        "outputId": "6414c382-8eef-47aa-f9ab-5e05ded8119a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.510825457099338\n",
            "2.302584092994546\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2025-05-20 외박 및 당직이슈.\n",
        "#미니배치 학습\n",
        "#지금까지 데이터 하나의 대한 손실함수만 생각해왔으니, 이제 훈련 데이터 모두에 대한 손실함수의 합을 구해보자\n",
        "import numpy as np\n",
        "import importlib\n",
        "import sys\n",
        "import pickle\n",
        "importlib.invalidate_caches()\n",
        "sys.path.append('/content/drive/MyDrive/dataset')\n",
        "\n",
        "from mnist import load_mnist\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize= True, one_hot_label= True) #정규화 on 원하라벨 = 정답 위치 원소만1\n",
        "\n",
        "print(x_train.shape) # 784개의 픽셀의 가진 이미지 6만개 학습용 데이터\n",
        "print(t_train.shape) # 그에 대응 되는 정답 데이터 6만개.\n",
        "print(x_test.shape) #여긴 테스트 데이터 1만개\n",
        "print(t_test.shape)"
      ],
      "metadata": {
        "id": "-g7k4ZUc9wG7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff017f4-4656-4526-fb40-df598c2b08d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n",
            "(10000, 784)\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#위의 많은 데이터중 10장만 빼내는법\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 10\n",
        "batch_mask = np.random.choice(train_size, batch_size) # 0~6만 수중에 무작위 10개 뽑\n",
        "x_batch = x_train[batch_mask]\n",
        "t_batch = t_train[batch_mask]"
      ],
      "metadata": {
        "id": "_kM0J5-l_uSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#배치용 교차 엔트로피 오차 구현하기\n",
        "\n",
        "def cross_entropy_error(y,t):\n",
        "  if y.ndim == 1: #1차원이라면.\n",
        "    t = t.reshape(1,t.size)\n",
        "    y = y.reshape(1,y.size) # y가 1차원이라면, 즉 데이터 하나에 대한 오차를 구하는경우 데이터의 형상을 바꿈\n",
        "    #(3,)같은 형태를 (1,3)으로 바꾼다.\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(t * np.log(y + 1e-7)) / batch_size\n"
      ],
      "metadata": {
        "id": "sTHDtIqcL1Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2025-05-21\n",
        "#수치미분\n",
        "\n",
        "#나쁜 구현의 예\n",
        "def numerical_diff(f,x):\n",
        "  h = 10e-50\n",
        "  return (f(x+h) - f(x) / h) #10e-50을 컴퓨터에서는 표현을 못함. 그래서 미분구현이 살짝 다름\n",
        "\n",
        "#옳은 구현\n",
        "def numerical_diff(f,x):\n",
        "  h = 1e-4\n",
        "  return (f(x+h) - f(x-h)) / (2*h)"
      ],
      "metadata": {
        "id": "GfAvC0cmOrj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y = 0.01x^2 + 0.1x 미분\n",
        "def function_1(x):\n",
        "  return 0.01*x**2 + 0.1*x\n",
        "\n",
        "numerical_diff(function_1,5) #0.19999999999990898\n",
        "\n",
        "numerical_diff(function_1, 10) #0.2999999999986347\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntLOJh6JjL_1",
        "outputId": "333b6437-6c7a-4e62-b823-716a0da4b1f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2999999999986347"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#편미분은 어떻게?\n",
        "#x0에 대한 편미분을 구하고 싶으면 x만 변수로 놔두고 x1은 상수로 바꿔버리면 된다.\n",
        "\n",
        "#x0 = 3 x1 = 4일때 x0에 대한 편미분\n",
        "def function_tmp1(x0):\n",
        "  return x0*x0 + 4.0**2.0\n",
        "\n",
        "numerical_diff(function_tmp1,3.0) #6.00000000000378\n",
        "\n",
        "#x0 = 3 x1 = 4일때 x1에 대한 편미분\n",
        "def function_tmp2(x1):\n",
        "  return 3.0**2 + x1**2.0\n",
        "\n",
        "numerical_diff(function_tmp1,4.0) #7.999999999999119\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4u5TY83akQKJ",
        "outputId": "03827a1b-580a-4a5f-f09f-04faaeba346f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.999999999999119"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#위에서 x0과 x1의 편미분을 변수별로 따로 계산했는데 x0과 x1의 편미분을 동시에 계산할려면?\n",
        "#기울기 벡터 구하는법\n",
        "\n",
        "def numerical_gradient(f,x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x)\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]\n",
        "    #f(x+h)계산\n",
        "    x[idx] = tmp_val + h\n",
        "    fxh1 = f(x)\n",
        "#2025-05-24\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x)\n",
        "    grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "    #아까 위에서 구현한 수치미분이랑 동일함.\n",
        "\n",
        "  return grad\n",
        "\n",
        "def func_ex2(x):\n",
        "  return x[0] ** 2 + x[1]**2\n",
        "\n",
        "numerical_gradient(func_ex2,np.array([3.0,4.0])) # 정상적으로 6과 8을 출력하는 모습"
      ],
      "metadata": {
        "id": "-n7Dnaq0oBpP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dac10224-1e21-498f-cb8e-a63bebe9b261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6., 8.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#경사하강법 gradient descent\n",
        "#기계학습 문제 대부분은 학습 단계에서 최적의 매개변수를 찾아내는데 신경망도 똑같다. 여기서 최적이란\n",
        "#손실함수가 최솟값이 될 때의 매개변수의 값인데 이걸 찾아내는게 경사하강법이다.\n",
        "# 경사하강법의 식 x[0] = x[0] - α(∂f/∂x[0]) , x[1] = x[1] - α(∂f/∂x[1])\n",
        "#α = 하이퍼 파라미터인 러닝레이트 학습 속도를 조절해준다.\n",
        "\n",
        "def gradient_descent(f,init_x,lr = 0.01,step_num = 100): #d init_x = 초깃값,러닝레이트 0.01 반복횟수는 100번\n",
        "  x = init_x\n",
        "\n",
        "  for i in range(step_num):\n",
        "    grad = numerical_gradient(f,x)\n",
        "    x -= lr*grad\n",
        "\n",
        "  return x\n",
        "\n",
        "#문제 : 경사하강법으로f(x[0],x[1]) = x[0]^2 + x[1]^2의 최솟값을 구하라.\n",
        "#일단 답은 당연히 [0,0]임\n",
        "\n",
        "def function_2(x):\n",
        "  return x[0] ** 2 + x[1] **2\n",
        "\n",
        "gradient_descent(function_2, init_x= np.array([-3.0,4.0]),lr = 0.1,step_num=100) # [-0.0005, -0.0005]"
      ],
      "metadata": {
        "id": "ZVTgusjpyoSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c6df426-b913-47a3-d776-108f0124bc76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0005, -0.0005])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#신경망에서의 기울기\n",
        "#신경망 학습에서도 기울기를 구해야 하는데, 여기서 말하는 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다\n",
        "#걍 간단하게 말하면 손실함수를 미분하여 w파라미터를 업데이트한다는 소리\n",
        "#예를 들어 형상이 2x3, 가중치가 W(벡터), 손실 함수가 L인 신경망을 예시로 들어보면\n",
        "#∂L/∂W (각 원소에 대한 편미분 예를들어 1행 1번째 원소인 (∂L/∂w11)는 w11를 조금 변경했을때 손실함수 L이 얼마나 변하나임\n",
        "# W = np.array([[w11,w12,w13],[w21,w22,w23]])\n",
        "\n",
        "#간단한 실제 기울기 구하는 예제\n",
        "\n",
        "from func import softmax, cross_entropy_error, sigmoid, numerical_gradient\n",
        "\n",
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2,3) #정규분포로 초기화\n",
        "\n",
        "  def predict(self,x):\n",
        "    return np.dot(x,self.W)\n",
        "\n",
        "  def loss(self, x,t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y,t)\n",
        "\n",
        "    return loss\n",
        "\n",
        "net = simpleNet()\n",
        "print(net.W)\n",
        "\n",
        "x = np.array([0.6,0.9])\n",
        "p = net.predict(x)\n",
        "print(p)\n",
        "\n",
        "np.argmax(p)\n",
        "\n",
        "t = np.array([0,0,1]) #원 핫 코딩\n",
        "print(net.loss(x,t))\n",
        "\n",
        "def f(W):\n",
        "  return net.loss(x,t)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "spJ_2osw4hJr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5d56f21-8bd7-4304-d084-47d591aad3be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.64684821 -1.71638144 -0.28383369]\n",
            " [ 0.73393708  2.06827318  0.67187364]]\n",
            "[1.04865229 0.831617   0.43438606]\n",
            "1.4669516813031438\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2025-05-25\n",
        "#2층 신경망 클래스 구현\n",
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):#입력층,은닉층,출력층의뉴련수\n",
        "    #가중치 초기화\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  def perdict(self,x):\n",
        "    W1,W2 = self.params['W1'], self.params['W2']\n",
        "    b1,b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x,W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "    a2 = np.dot(z1,W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    return y\n",
        "\n",
        "  def loss(self,x,t):\n",
        "    y = self.perdict(x)\n",
        "\n",
        "    return cross_entropy_error(y,t)\n",
        "\n",
        "  def accuracy(self,x,t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis = 1)\n",
        "    t = np.argmax(t, axis = 1)\n",
        "\n",
        "    accuracy = np.sum(y==t) / float(x.shape[0])\n",
        "\n",
        "    return accuracy\n",
        "#x = 입력 t = 정답 레이블\n",
        "  def numerical_gradient(self,x,t):\n",
        "    loss_W = lambda W: self.loss(x,t)\n",
        "    grads = {}\n",
        "    # 기울기 벡터 구하는 부분\n",
        "    grads['W1'] = numerical_gradient(loss_W,self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W,self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W,self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W,self.params['b2'])\n",
        "    #numerical_gradient함수의 인자로 loss함수와 W1값을 줘서 W1의 원소값을 h만큼 늘리면 얼마나 오차가 생기는지로\n",
        "    #기울기를 측정함 이를 기반으로 기울기 벡터를 구함.\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "g3mMifsF93JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = TwoLayerNet(input_size=784,hidden_size=100,output_size=10)\n",
        "\n",
        "x = np.random.rand(100,784) #더미 데이터 100장\n",
        "t = np.random.rand(100,10) #정답 데이터 100장\n",
        "\n",
        "y = net.perdict(x)\n",
        "\n",
        "grads = net.numerical_gradient(x,t) #약 3분이나 걸린다.\n",
        "#역전파를 배우면 고속으로 기울기 벡터를 찾는 함수를 만들것이다."
      ],
      "metadata": {
        "id": "vi_a_acaEJQy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}