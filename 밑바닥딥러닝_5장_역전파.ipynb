{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tYdgjqCowtUCyTQWm3EGXdj5JA0PbI83",
      "authorship_tag": "ABX9TyPibC/1VHJN5A462IGi5SN8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinjeongdong/MLDeeplearningStudy/blob/main/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%94%A5%EB%9F%AC%EB%8B%9D_5%EC%9E%A5_%EC%97%AD%EC%A0%84%ED%8C%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import importlib\n",
        "import sys\n",
        "import pickle\n",
        "import copy\n",
        "importlib.invalidate_caches()\n",
        "sys.path.append('/content/drive/MyDrive/dataset')\n",
        "from func import softmax,cross_entropy_error"
      ],
      "metadata": {
        "id": "Lb2Sfiy1fgt5"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#역전파는 신경망의 각 노드가 가지고 있는 가중치(Weight)와 편향(Bias)을 학습시키기 위한 알고리즘으로, 딥러닝에 있어서 가장 핵심적인 부분이라고 할 수 있다.\n",
        "#목표(Target)와 모델의 예측 결과(Output)가 얼마나 차이가 나는지 확인하고 그 오차를 바탕으로 가중치와 편향을 뒤에서부터 앞으로 갱신해가는 것을 의미한다.\n",
        "#역전파란 명칭도 바로 이처럼 뒤에서부터 다시 앞으로 거슬러 올라간다는 것에서 나온 것이다.\n",
        "#핵심은 chain rule 합성함수의 미분법을 활용하는것이다.\n",
        "#ex 손실함수 L이랑 활성함수 y_i = f(w_i)가 있을때 L(f(w_i))일때∂L/∂w_i을 구하고 싶을땐\n",
        "#∂L/∂y_i * ∂y_i/∂w_i을 하면 분수형태로 만들어져 ∂y_i ∂y_i는 지워져 ∂L/∂w_i만 남게 된다.\n",
        "#말로 하면 어려우니 역전파 관련 유튜브를 찾아보는걸 추천\n",
        "#∂"
      ],
      "metadata": {
        "id": "CRKGOSa6A9Gf"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#활성함수 계층 구현하기\n",
        "#Relu y = x (x>0) else 0\n",
        "#∂y/∂x if x>0,1 else, 0\n",
        "\n",
        "class Relu:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    self.mask = (x<=0)\n",
        "    out = x.copy()\n",
        "\n",
        "    out[self.mask] = 0\n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "    dout[self.mask] = 0\n",
        "    dx = dout\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "4oGznV0zCPBv"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sigmoid\n",
        "#1 / 1+exp(-x) 역전파 = ∂L/∂y * sigmoid미분\n",
        "\n",
        "class sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self,x):\n",
        "    out = 1 / (1+np.exp(-x))\n",
        "    self.out = out\n",
        "    return out\n",
        "\n",
        "  def backward(self,dout):\n",
        "\n",
        "    dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "    return dx"
      ],
      "metadata": {
        "id": "0as6v8v8Et-1"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#배치용 Affine 계층\n",
        "\n",
        "\n",
        "class Affine:\n",
        "  def __init__(self,W,b):\n",
        "    self.W = W\n",
        "    self.b = b\n",
        "    self.x = None\n",
        "    self.dW = None\n",
        "    self.db = None\n",
        "  def forward(self,x):#순전파\n",
        "    self.x = x\n",
        "    out = np.dot(x,self.W) + self.b  #x가 들어오면 W랑 행렬곱 + b을해준다\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self,dout): #역전파\n",
        "    dx = np.dot(dout, self.W.T) #∂L/∂x 구하는법 미분값 * W의 T를 곱해준다.\n",
        "\n",
        "    self.dW = np.dot(self.x.T,dout) #W의 기울기 벡터는 x.T dout를 곱해준다.\n",
        "    self.db = np.sum(dout,axis = 0)\n",
        "\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "xT3NW9jmKsPG"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#softmax + loss\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "  def __init__(self):\n",
        "    self.loss = None # 손실\n",
        "    self.y =  None # softmax의 출력 = 예측값\n",
        "    self.t = None #정답 데이터 원핫 인코딩\n",
        "\n",
        "  def forward(self,x,t): #소프트맥스와 손실함수\n",
        "    self.t = t #정답데이터\n",
        "    self.y = softmax(x) #Affine2 순전파가 완료된 값을 softmax로 각각의 확률을 구해준다.\n",
        "    self.loss = cross_entropy_error(self.y,self.t) #그리고 예측값과 정답값을 넣어줘 손실을 구해준다.\n",
        "    return self.loss\n",
        "\n",
        "  def backward(self, dout = 1):\n",
        "    batch_size = self.t.shape[0]\n",
        "    dx = (self.y - self.t) / batch_size #Softmax랑 로스함수의 역전파는 (y_i - t_i)이다 간단\n",
        "\n",
        "    return dx\n"
      ],
      "metadata": {
        "id": "NNFwH8FmN8gd"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#역전파를 이용한 신경망 구현 알고리즘\n",
        "#1단계 미니배치 훈련 데이터중 일부를 무작위로 가져옴 이 미니배치의 손실함수 값을 줄이는게 목표\n",
        "#2단계 기울기 산출 미니배치의 손실함수를 줄이기 위해 기울기벡터를 구해준다\n",
        "#3단계 가중치를 기울기방향으로 조금 갱신한다\n",
        "#4단계 1~3단계 반복\n",
        "#일단 2층 신경망을 구현해보자\n",
        "from func import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "class TwoLayerNet:\n",
        "  def __init__(self,input_size,hidden_size,output_size,weight_init_std = 0.01):\n",
        "    self.params = {}\n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size,hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size,output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    #계층생성\n",
        "    self.layers = OrderedDict() #딕셔너리 생성\n",
        "    self.layers['Affine1'] = Affine(self.params['W1'],self.params['b1']) #클래스를 생성해줘서 W1 b1 인스턴스 변수들을 할당한다.\n",
        "    self.layers['Relu1'] = Relu() #클래스를 생성해줘서 인스턴스 변수들을 할당한다.\n",
        "    self.layers['Affine2'] = Affine(self.params['W2'],self.params['b2']) #클래스를 생성해줘서 W2 b2 인스턴스 변수들을 할당한다.\n",
        "\n",
        "    self.lastlayer = SoftmaxWithLoss() #이 Affine2까지 진행된 순전파를 이용해 예측 확률값과 손실값 인스턴스 변수를 만든다.\n",
        "\n",
        "  def predict(self,x):\n",
        "    for layer in self.layers.values(): #순전파 부분.\n",
        "\n",
        "      x = layer.forward(x) #affine 1 relu1 affine2 순서로 순전파를 함수를 실행해준다.\n",
        "\n",
        "    return x\n",
        "\n",
        "  def loss(self,x,t):\n",
        "    y = self.predict(x) #위의 layers의 순전파를 실행\n",
        "\n",
        "    return self.lastlayer.forward(y,t) # softmaxwithloss 실행\n",
        "\n",
        "  def accuracy(self,x,t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y,axis=1)\n",
        "    if t.ndim != 1 : t = np.argmax(t,axis = 1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  def numerical_gradient(self,x,t): #전 쳅터에서 구현한 부분.\n",
        "    loss_W = lambda W: self.loss(x,t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads\n",
        "\n",
        "\n",
        "  def gradient(self,x,t):\n",
        "    #순전파\n",
        "    self.loss(x,t) # loss를 실행하여 Affine1, relu, Affine2함수의 모든 인스턴스 변수를 갱신해준다.\n",
        "\n",
        "\n",
        "    #역전파\n",
        "\n",
        "    dout = 1\n",
        "\n",
        "    dout = self.lastlayer.backward(dout) #softmaxwithloss의 역전파를 실행한다. (y-t)\n",
        "\n",
        "    layers = list(self.layers.values())\n",
        "    layers.reverse() #역전파는 순전파의 반대방향으로 실행함 Affine2 -> Relu1 -> Affine1 순으로 그래서 역전환 함.\n",
        "\n",
        "    for layer in layers:\n",
        "      dout = layer.backward(dout) #반대로 만든것을 실행.\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = self.layers['Affine1'].dW #Affine1또는2 에 있는 인스턴스 변수인 W,b의 기울기 이하 동문.\n",
        "    grads['b1'] = self.layers['Affine1'].db\n",
        "    grads['W2'] = self.layers['Affine2'].dW\n",
        "    grads['b2'] = self.layers['Affine2'].db\n",
        "\n",
        "    return grads"
      ],
      "metadata": {
        "id": "LyrCIldJPTF8"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mnist import load_mnist\n",
        "\n",
        "(x_train,t_train),(x_test,t_test) = load_mnist(normalize=True,one_hot_label=True) #각각의 데이터는 (60000,784), (60000,100), (10000,784), (10000,100) 형식\n",
        "\n",
        "network = TwoLayerNet(784,50,10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size,1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  batch_mask = np.random.choice(train_size,batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  grad = network.gradient(x_batch,t_batch)\n",
        "\n",
        "  for key in ('W1','b1','W2','b2'):\n",
        "    network.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "  loss = network.loss(x_batch,t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  if i% iter_per_epoch ==0:\n",
        "    train_acc = network.accuracy(x_train,t_train)\n",
        "    test_acc = network.accuracy(x_test,t_test)\n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "    print(train_acc,test_acc) # 0.09843333333333333 0.1031 -> 0.9781333333333333 0.9702"
      ],
      "metadata": {
        "id": "TR6qShRdvuGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c58353-469c-42f5-ec8d-56d975dbe88f"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09843333333333333 0.1031\n",
            "0.9037666666666667 0.9097\n",
            "0.9239833333333334 0.9267\n",
            "0.93425 0.935\n",
            "0.94245 0.9389\n",
            "0.9480166666666666 0.9456\n",
            "0.9550333333333333 0.952\n",
            "0.9589333333333333 0.9562\n",
            "0.9628666666666666 0.9584\n",
            "0.96555 0.9604\n",
            "0.9668833333333333 0.9625\n",
            "0.96895 0.9632\n",
            "0.9725166666666667 0.9653\n",
            "0.9735 0.9659\n",
            "0.9758666666666667 0.9679\n",
            "0.9767166666666667 0.9684\n",
            "0.9781333333333333 0.9702\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#간단한 테스트\n",
        "net = TwoLayerNet(2,3,2)\n",
        "\n",
        "net.params['W1'] = np.array([[0.1,0.2,0.3],[0.6,0.5,0.4]])\n",
        "\n",
        "net.params['W2'] = np.array([[0.3,0.1],[0.2,0.5],[0.8,0.7]])\n",
        "\n",
        "\n",
        "x = np.array([[2,3]])\n",
        "t = np.array([[0,1]])\n",
        "\n",
        "net.layers = OrderedDict() #딕셔너리 생성\n",
        "net.layers['Affine1'] = Affine(net.params['W1'],net.params['b1']) #클래스를 생성해줘서 W1 b1 인스턴스 변수들을 할당한다.\n",
        "net.layers['Relu1'] = Relu() #클래스를 생성해줘서 인스턴스 변수들을 할당한다.\n",
        "net.layers['Affine2'] = Affine(net.params['W2'],net.params['b2']) #클래스를 생성해줘서 W2 b2 인스턴스 변수들을 할당한다.\n",
        "\n",
        "net.loss(x,t) #0.698159479502866\n",
        "\n",
        "gd = net.gradient(x,t)\n",
        "\n",
        "net.params['W1'] -= 1 * gd['W1']\n",
        "\n",
        "net.loss(x,t)\n",
        "\n",
        "\n",
        "for i in range(0,9):\n",
        "  net.params['W1'] -= 1 * gd['W1']\n",
        "\n",
        "\n",
        "\n",
        "net.loss(x,t)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJHI2L6zPLDy",
        "outputId": "d0372c22-506f-4a06-9104-0f01a50f8dee"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.0015803586491286087)"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    }
  ]
}